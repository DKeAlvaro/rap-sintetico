{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Entrenamiento de \"Baby GPT\" (Corregido GPU + ONNX)\n",
                "\n",
                "Este notebook entrena un un modelo GPT-2 minúsculo (<10MB) desde cero y lo exporta a ONNX para móviles."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Instalación de librerías\n",
                "%pip install transformers datasets tokenizers torch accelerate onnx onnxruntime"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import json\n",
                "import os\n",
                "from tokenizers import ByteLevelBPETokenizer\n",
                "from transformers import GPT2Config, GPT2LMHeadModel, GPT2TokenizerFast, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
                "from datasets import Dataset\n",
                "\n",
                "# Verificar hardware\n",
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "print(f\"Usando: {device.upper()}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Generar datos y Tokenizer personalizado\n",
                "text_data = []\n",
                "if os.path.exists(\"dataset.jsonl\"):\n",
                "    with open(\"dataset.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
                "        for line in f:\n",
                "            try:\n",
                "                obj = json.loads(line)\n",
                "                text_data.append(f\"TEMA: {obj['topic']} \\nRIMA: {obj['rap']}\")\n",
                "            except: pass\n",
                "else:\n",
                "    # Datos dummy por si no existe el fichero al probar\n",
                "    text_data = [\"TEMA: Prueba \\nRIMA: Esto es una prueba, no te muevas.\"] * 100\n",
                "\n",
                "# Guardar txt temporal para entrenar tokenizer\n",
                "with open(\"train_text.txt\", \"w\", encoding=\"utf-8\") as f:\n",
                "    f.write(\"\\n\".join(text_data))\n",
                "\n",
                "# Entrenar Tokenizer BPE (Byte-Pair Encoding)\n",
                "tokenizer = ByteLevelBPETokenizer()\n",
                "tokenizer.train(files=[\"train_text.txt\"], vocab_size=5000, min_frequency=2, special_tokens=[\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<mask>\"])\n",
                "\n",
                "os.makedirs(\"./baby_rhyme_gpt\", exist_ok=True)\n",
                "tokenizer.save_model(\"./baby_rhyme_gpt\")\n",
                "\n",
                "# Cargar como GPT2TokenizerFast (wrapper de HuggingFace)\n",
                "tokenizer_gpt = GPT2TokenizerFast.from_pretrained(\"./baby_rhyme_gpt\")\n",
                "tokenizer_gpt.add_special_tokens({'eos_token': '</s>', 'bos_token': '<s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'mask_token': '<mask>'})"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Configurar Modelo Mini-GPT\n",
                "config = GPT2Config(\n",
                "    vocab_size=5002,\n",
                "    n_positions=256,\n",
                "    n_embd=128,       # Muy pequeño para que pese poco\n",
                "    n_layer=4,        # Pocas capas\n",
                "    n_head=4,\n",
                "    bos_token_id=tokenizer_gpt.bos_token_id,\n",
                "    eos_token_id=tokenizer_gpt.eos_token_id,\n",
                ")\n",
                "\n",
                "model = GPT2LMHeadModel(config).to(device)\n",
                "print(f\"Tamaño del modelo: {sum(p.numel() for p in model.parameters())/1e6:.2f} M parámetros\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. Preparar Dataset\n",
                "raw_dataset = Dataset.from_dict({\"text\": text_data})\n",
                "\n",
                "def tokenize_function(examples):\n",
                "    return tokenizer_gpt(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
                "\n",
                "# Tokenizamos y ELIMINAMOS la columna de texto original para evitar errores en el Trainer\n",
                "tokenized_datasets = raw_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
                "\n",
                "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer_gpt, mlm=False)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4. Entrenamiento\n",
                "training_args = TrainingArguments(\n",
                "    output_dir=\"./baby_rhyme_gpt_checkpoints\",\n",
                "    overwrite_output_dir=True,\n",
                "    num_train_epochs=50,\n",
                "    per_device_train_batch_size=64,\n",
                "    learning_rate=1e-3,\n",
                "    save_steps=1000,\n",
                "    logging_steps=100,\n",
                "    prediction_loss_only=True,\n",
                "    fp16=torch.cuda.is_available(), # Mixed Precision para velocidad\n",
                "    remove_unused_columns=False     # Evita otro tipo de errores de columnas\n",
                ")\n",
                "\n",
                "trainer = Trainer(\n",
                "    model=model,\n",
                "    args=training_args,\n",
                "    data_collator=data_collator,\n",
                "    train_dataset=tokenized_datasets,\n",
                ")\n",
                "\n",
                "trainer.train()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 5. Guardar y Exportar a ONNX\n",
                "output_path = \"./baby_rhyme_gpt\"\n",
                "model.save_pretrained(output_path)\n",
                "tokenizer_gpt.save_pretrained(output_path)\n",
                "\n",
                "print(\"Modelo guardado. Exportando a ONNX...\")\n",
                "\n",
                "# Exportar a ONNX\n",
                "dummy_input = tokenizer_gpt(\"TEMA: Test\", return_tensors=\"pt\").input_ids.to(device)\n",
                "onnx_path = os.path.join(output_path, \"model.onnx\")\n",
                "\n",
                "torch.onnx.export(\n",
                "    model,\n",
                "    dummy_input,\n",
                "    onnx_path,\n",
                "    opset_version=14,\n",
                "    input_names=['input_ids'],\n",
                "    output_names=['logits'],\n",
                "    dynamic_axes={'input_ids': {0: 'batch_size', 1: 'seq_len'}, 'logits': {0: 'batch_size', 1: 'seq_len'}}\n",
                ")\n",
                "print(f\"ONNX exportado a: {onnx_path}\")\n",
                "\n",
                "# Cuantizar para móvil (8-bit)\n",
                "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
                "quant_path = os.path.join(output_path, \"model_quant.onnx\")\n",
                "quantize_dynamic(onnx_path, quant_path, weight_type=QuantType.QUInt8)\n",
                "print(f\"Modelo cuantizado para móvil listo en: {quant_path}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 6. Prueba Final (Inferencia)\n",
                "from transformers import pipeline\n",
                "\n",
                "# Usamos CPU para inferencia simple de prueba para asegurar que carga bien\n",
                "generator = pipeline(\"text-generation\", model=output_path, tokenizer=output_path, device=-1)\n",
                "\n",
                "def rima(tema):\n",
                "    prompt = f\"TEMA: {tema} \\nRIMA:\"\n",
                "    out = generator(prompt, max_length=60, num_return_sequences=1, do_sample=True, temperature=0.7)\n",
                "    print(out[0]['generated_text'])\n",
                "    print(\"-\"*10)\n",
                "\n",
                "rima(\"Amor\")\n",
                "rima(\"Futuro\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
